{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "def readData(): \n",
    "    dataA, dataB, dataC = [], [], []\n",
    "    path = 'data/data/'\n",
    "    \n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'train_targets.parquet')))\n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'X_test_estimated.parquet')))\n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'X_train_estimated.parquet')))\n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'X_train_observed.parquet')))        \n",
    "   \n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'train_targets.parquet')))\n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'X_test_estimated.parquet')))\n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'X_train_estimated.parquet')))\n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'X_train_observed.parquet')))\n",
    "\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'train_targets.parquet')))\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'X_test_estimated.parquet')))\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'X_train_estimated.parquet')))\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'X_train_observed.parquet')))\n",
    "   \n",
    "    return dataA, dataB, dataC\n",
    "\n",
    "A, B, C = readData()\n",
    "\n",
    "def splitWeatherAndEnergyReports(data):\n",
    "    weather = [data[1], data[2], data[3]]\n",
    "    energy = data[0]\n",
    "    return weather, energy\n",
    "\n",
    "def quartersToHours(data):\n",
    "    data1 = []\n",
    "    for df in data:\n",
    "        df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
    "        df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
    "        df_new['date_forecast'] = df_new['date_forecast'].apply(lambda x: x + ':00:00')\n",
    "        data1.append(df_new)     \n",
    "    return data1\n",
    "\n",
    "#A\n",
    "X = pd.concat([A[3], A[2]], ignore_index = True)\n",
    "X = pd.concat([X, A[1]], ignore_index = True)\n",
    "X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "X[['ceiling_height_agl:m', 'cloud_base_agl:m']] = X[['ceiling_height_agl:m', 'cloud_base_agl:m']].interpolate(method='cubic')\n",
    "A[3], A[2], A[1] = X[:len(A[3])], X[len(A[3]):len(A[3])+len(A[2])], X[len(A[2])+len(A[3]):]\n",
    "\n",
    "\n",
    "#B\n",
    "X = pd.concat([B[3], B[2]], ignore_index = True)\n",
    "X = pd.concat([X, B[1]], ignore_index = True)\n",
    "X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "X[['ceiling_height_agl:m', 'cloud_base_agl:m']] = X[['ceiling_height_agl:m', 'cloud_base_agl:m']].interpolate(method='cubic')\n",
    "B[3], B[2], B[1] = X[:len(B[3])], X[len(B[3]):len(B[3])+len(B[2])], X[len(B[2])+len(B[3]):]\n",
    "\n",
    "\n",
    "#C\n",
    "X = pd.concat([C[3], C[2]], ignore_index = True)\n",
    "X = pd.concat([X, C[1]], ignore_index = True)\n",
    "X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "X[['ceiling_height_agl:m', 'cloud_base_agl:m']] = X[['ceiling_height_agl:m', 'cloud_base_agl:m']].interpolate(method='cubic')\n",
    "\n",
    "C[3], C[2], C[1] = X[:len(C[3])], X[len(C[3]):len(C[3])+len(C[2])], X[len(C[2])+len(C[3]):]\n",
    "\n",
    "#Splitting weather and energy datasets\n",
    "weather_A, energy_A = splitWeatherAndEnergyReports(A)\n",
    "weather_B, energy_B = splitWeatherAndEnergyReports(B)\n",
    "weather_C, energy_C = splitWeatherAndEnergyReports(C)\n",
    "\n",
    "#Joining rows from same hour.\n",
    "weather_A1 = quartersToHours(weather_A)\n",
    "weather_B1 = quartersToHours(weather_B)\n",
    "weather_C1 = quartersToHours(weather_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B - Converting rows in B that is incorrect to NAN, to later be removed. \n",
    "energy_B['flag'] = energy_B['pv_measurement'].groupby([energy_B['pv_measurement'], energy_B['pv_measurement'].diff().ne(0).cumsum()]).transform('size').ge(24).astype(int) \n",
    "energy_B.loc[energy_B['flag'] == 1, 'pv_measurement'] = None\n",
    "energy_B = energy_B.drop(['flag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#DROP NAN-ROWS\n",
    "def dropNanrows(df):\n",
    "    return df.dropna()\n",
    "\n",
    "#Block B&C has label-rows with NAN-values that needs to be dropped.\n",
    "energy_B  = dropNanrows(energy_B)\n",
    "energy_C = dropNanrows(energy_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align rows\n",
    "def alignData(x_train, x_test, y):     \n",
    "    X = pd.concat([x_train, x_test], ignore_index = True)\n",
    "    y = y.rename(columns={'time': 'date_forecast'})\n",
    "    y['date_forecast'] = y['date_forecast'].astype(str)\n",
    "    aligned = X.merge(y, how='inner', on=['date_forecast'])\n",
    "\n",
    "    X = aligned.drop(['pv_measurement'], axis=1)\n",
    "    Y = aligned['pv_measurement']\n",
    "    return X, Y\n",
    "\n",
    "#A-BLOCK\n",
    "X_A, Y_A = alignData(weather_A1[2], weather_A1[1], energy_A) \n",
    "testData_A = weather_A1[0] \n",
    "\n",
    "#B-BLOCK\n",
    "X_B, Y_B = alignData(weather_B1[2], weather_B1[1], energy_B)\n",
    "testData_B = weather_B1[0]\n",
    "\n",
    "#C-BLOCK\n",
    "X_C, Y_C = alignData(weather_C1[2], weather_C1[1], energy_C)\n",
    "testData_C = weather_C1[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing functions (currently in use)\n",
    "\n",
    "def normalizeAll(tr, te):\n",
    "    all = pd.concat([tr, te], ignore_index = True)\n",
    "    #minmax = [(all[str(col)].min(), all[str(col)].max()) for col in all.columns]\n",
    "    all = all.apply(lambda iterator: ((iterator.max() - iterator)/(iterator.max() - iterator.min())))\n",
    "    return all[:len(tr)], all[len(tr):]\n",
    "\n",
    "def dataSplit2(X_tree, X_NN, Y, td_tree, td_NN, splitRatio):\n",
    "        X_tree['Y'] = pd.DataFrame(Y)['pv_measurement']\n",
    "        tr_trees, te_trees = train_test_split(X_tree, test_size=splitRatio)\n",
    "        \n",
    "        Y_tr_trees = tr_trees['Y']\n",
    "        Y_te_trees = te_trees['Y']\n",
    "\n",
    "        tr_trees.drop(['Y'], axis=1, inplace=True)\n",
    "        te_trees.drop(['Y'], axis=1, inplace=True)\n",
    "\n",
    "        X_NN['Y'] = pd.DataFrame(Y)['pv_measurement']\n",
    "        tr_NN, te_NN = train_test_split(X_NN, test_size=splitRatio)\n",
    "        \n",
    "        Y_tr_NN = tr_NN['Y']\n",
    "        Y_te_NN = te_NN['Y']\n",
    "\n",
    "        tr_NN.drop(['Y'], axis=1, inplace=True)\n",
    "        te_NN.drop(['Y'], axis=1, inplace=True)\n",
    "\n",
    "        return tr_trees, te_trees, tr_NN, te_NN, Y_tr_trees, Y_te_trees, Y_tr_NN, Y_te_NN, td_tree, td_NN\n",
    "\n",
    "def add_times(tr, te):\n",
    "    all = pd.concat([tr, te], ignore_index = True) \n",
    "    all[\"Hour\"] = [int(all.iloc[i]['date_forecast'][11:13]) for i in range(len(all))]\n",
    "    all[\"Month\"] = [int(all.iloc[i]['date_forecast'][5:7]) for i in range(len(all))]\n",
    "    all[\"Year\"] = [int(all.iloc[i]['date_forecast'][0:4]) for i in range(len(all))]\n",
    "    all[\"Day\"] = [int(all.iloc[i]['date_forecast'][8:10]) for i in range(len(all))]\n",
    "    all['Week'] = pd.to_datetime(all['date_forecast']).dt.isocalendar().week.astype(float)\n",
    "    return all[:len(tr)], all[len(tr):] \n",
    "   \n",
    "def removeDates(X):\n",
    "    X = X.drop(['date_forecast'], axis=1)\n",
    "    return X\n",
    "\n",
    "def removeColumns(df, columns): \n",
    "    return df.drop(columns, axis=1)  \n",
    "\n",
    "def flag(tr, te, splitIndex):\n",
    "    data_observed = tr.iloc[:splitIndex]\n",
    "    data_estimated = tr.iloc[splitIndex:]\n",
    "    \n",
    "    data_observed['flag'] = 1\n",
    "    data_estimated['flag'] = 0\n",
    "    te['flag'] = 0\n",
    "    return pd.concat([data_observed, data_estimated], ignore_index = True), te\n",
    "\n",
    "def createFeatures(tr, te,): \n",
    "    all = pd.concat([tr, te], ignore_index = True)\n",
    "    top5 = ['absolute_humidity_2m:gm3', 'ceiling_height_agl:m', 'air_density_2m:kgm3', 'cloud_base_agl:m', 'sun_azimuth:d'] \n",
    "    seenKombos = []\n",
    "    toCombine = [('effective_cloud_cover:p', 'clear_sky_energy_1h:J')]\n",
    "    for elem in top5:\n",
    "        for elem2 in top5:\n",
    "            if {elem, elem2} not in seenKombos:\n",
    "               seenKombos.append({elem, elem2})\n",
    "               if elem != elem2:\n",
    "                    toCombine.append((str(elem), str(elem2)))\n",
    "    toClose = []\n",
    "    for col in all.copy().columns:\n",
    "        for col2 in all.copy().columns:\n",
    "            if {str(col), str(col2)} not in seenKombos:\n",
    "                seenKombos.append({str(col), str(col2)})\n",
    "                if col != col2:\n",
    "                    if abs(all[str(col)].corr(all[str(col2)])) >= 0.8:\n",
    "                        if str(col)[:4] == str(col2)[:4]:\n",
    "                            toClose.append((str(col), str(col2)))\n",
    "                        elif str(col) in top5 or str(col2) in top5:\n",
    "                            toCombine.append((str(col), str(col2)))                       \n",
    "    created = []\n",
    "    for i in range(len(toCombine)):\n",
    "        all[toCombine[i][0] + '_' + toCombine[i][1]] = all[toCombine[i][0]] * all[toCombine[i][1]]\n",
    "        created.append(toCombine[i][0] + '_' + toCombine[i][1])\n",
    "    return all[:len(tr)], all[len(tr):], created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_observed['flag'] = 1\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_estimated['flag'] = 0\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_observed['flag'] = 1\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_estimated['flag'] = 0\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_observed['flag'] = 1\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_estimated['flag'] = 0\n"
     ]
    }
   ],
   "source": [
    "#Applying preprocessing functions to data-blocks.\n",
    "\n",
    "#DATA A\n",
    "splitIndex_A = X_A.index[X_A['date_forecast'] == '2022-10-28 22:00:00'][0]\n",
    "X_A, testData_A = flag(X_A, testData_A, splitIndex_A) #Observed data\n",
    "X_A, testData_A = add_times(X_A, testData_A)\n",
    "testData_A = removeDates(testData_A)\n",
    "X_A = removeColumns(X_A, X_A.columns.difference(testData_A.columns))  #Fitting trainingData to useful Features in testData\n",
    "X_A, testData_A, created = createFeatures(X_A, testData_A)\n",
    "X_A_neuralNets, testData_A_neuralNets = normalizeAll(X_A.copy(), testData_A.copy())\n",
    "X_A_tr_trees, X_A_te_trees,  X_A_tr_NN, X_A_te_NN, Y_A_tr_trees, Y_A_te_trees, Y_A_tr_NN, Y_A_te_NN, testData_A_tree, testData_A_NN = dataSplit2(X_A, X_A_neuralNets, Y_A, testData_A, \n",
    "                                                                                                                          testData_A_neuralNets, 0.15)\n",
    "#Features to remove (Products from createFeatures() that wasn't useful)\n",
    "X_A_tr_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_A_te_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_A_tr_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_A_te_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_A_tree.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_A_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#DATA B\n",
    "splitIndex_B = X_B.index[X_B['date_forecast'] == '2022-10-28 22:00:00'][0]\n",
    "X_B, testData_B = flag(X_B, testData_B, splitIndex_B)\n",
    "X_B, testData_B = add_times(X_B, testData_B )\n",
    "testData_B = removeDates(testData_B)\n",
    "X_B = removeColumns(X_B, X_B.columns.difference(testData_B.columns)) #Fitting trainingData to useful FEatures in testData\n",
    "X_B, testData_B, created = createFeatures(X_B, testData_B)\n",
    "X_B_neuralNets, testData_B_neuralNets = normalizeAll(X_B.copy(), testData_B.copy())\n",
    "X_B_tr_trees, X_B_te_trees,  X_B_tr_NN, X_B_te_NN, Y_B_tr_trees, Y_B_te_trees, Y_B_tr_NN, Y_B_te_NN, testData_B_tree, testData_B_NN = dataSplit2(X_B, X_B_neuralNets, Y_B, testData_B, \n",
    "                                                                                                                          testData_B_neuralNets, 0.15)\n",
    "#Features to remove (Products from createFeatures() that wasn't useful)\n",
    "X_B_tr_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_B_te_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_B_tr_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_B_te_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_B_tree.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_B_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#DATA C\n",
    "splitIndex_C = X_C.index[X_C['date_forecast'] == '2022-10-28 22:00:00'][0]\n",
    "X_C, testData_C = flag(X_C, testData_C, splitIndex_C)\n",
    "X_C, testData_C = add_times(X_C, testData_C)\n",
    "testData_C = removeDates(testData_C)\n",
    "X_C = removeColumns(X_C, X_C.columns.difference(testData_C.columns)) #Fitting trainingData to useful FEatures in testData\n",
    "X_C, testData_C, created = createFeatures(X_C, testData_C)\n",
    "X_C_neuralNets, testData_C_neuralNets = normalizeAll(X_C.copy(), testData_C.copy())\n",
    "X_C_tr_trees, X_C_te_trees,  X_C_tr_NN, X_C_te_NN, Y_C_tr_trees, Y_C_te_trees, Y_C_tr_NN, Y_C_te_NN, testData_C_tree, testData_C_NN = dataSplit2(X_C, X_C_neuralNets, Y_C, testData_C, \n",
    "                                                                                                                          testData_C_neuralNets, 0.15)\n",
    "#Features to remove (Products from createFeatures() that wasn't useful)\n",
    "X_C_tr_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_C_te_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_C_tr_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_C_te_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_C_tree.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_C_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTDATA\n",
    "testData_A = np.nan_to_num(testData_A_neuralNets.to_numpy(), nan=0)\n",
    "testData_B = np.nan_to_num(testData_B_neuralNets.to_numpy(), nan=0)\n",
    "testData_C = np.nan_to_num(testData_C_neuralNets.to_numpy(), nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a23c912d8e48039bfc2842ea549f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e21c78d61e348758c300e1a9b120b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551e9cb21179434bbdc01825269dc06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CATBOOST-MODEL\n",
    "def catBoost(iterations=10, depth=14, lr=0.1,  loss =\"MAE\"):\n",
    "  cat_model = CatBoostRegressor(iterations=iterations, \n",
    "                            depth=depth, \n",
    "                            learning_rate=lr, \n",
    "                            loss_function=loss,\n",
    "                            )\n",
    "  cat_model.fit(X_B_tr_trees, Y_B_tr_trees, eval_set=(X_B_te_trees, Y_B_te_trees), verbose=False, plot=True)\n",
    "  return cat_model\n",
    "\n",
    "#Train catboost-models\n",
    "catBoost_models = {'A': catBoost(iterations=3800, depth=12, lr=0.01, loss=\"MAE\"),\n",
    "                   'B': catBoost(iterations=30000, depth=9, lr=0.001, loss=\"MAE\"),\n",
    "                   'C': catBoost(iterations=30000, depth=9, lr=0.001, loss=\"MAE\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:9: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_A = np.column_stack((m for m in models_A))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:27: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_B = np.column_stack((m for m in models_B))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:43: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_C = np.column_stack((m for m in models_C))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:9: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_A = np.column_stack((m for m in models_A))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:27: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_B = np.column_stack((m for m in models_B))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:43: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_C = np.column_stack((m for m in models_C))\n"
     ]
    }
   ],
   "source": [
    "#SUBMISSIONS\n",
    "def submission(dict_models):\n",
    "    sample_submission = pd.read_csv('data/data/sample_submission.csv')\n",
    "\n",
    "    #A  \n",
    "    models_A = [m.predict(testData_A_tree) for m in dict_models['A']]\n",
    "    preds_A = np.column_stack([m for m in models_A])\n",
    "    meanPrediction_A = preds_A.mean(axis=1)\n",
    "    final_pred_A = [[p] for p in meanPrediction_A]\n",
    "\n",
    "\n",
    "    #B \n",
    "    models_B = [m.predict(testData_B_tree) for m in dict_models['B']]\n",
    "    preds_B = np.column_stack([m for m in models_B])\n",
    "    meanPrediction_B = preds_B.mean(axis=1)\n",
    "    final_pred_B = [[p] for p in meanPrediction_B]\n",
    "\n",
    "\n",
    "    #C \n",
    "    models_C = [m.predict(testData_C_tree) for m in dict_models['C']]\n",
    "    preds_C = np.column_stack([m for m in models_C])\n",
    "    meanPrediction_C = preds_C.mean(axis=1)\n",
    "    final_pred_C = [[p] for p in meanPrediction_C]\n",
    "\n",
    "\n",
    "\n",
    "    allPreds = np.append(final_pred_A, final_pred_B, axis=0)\n",
    "    allPreds = np.append(allPreds, final_pred_C, axis=0)\n",
    "\n",
    "    #Replace potential negative values with zero, as energy production cannot be negative.\n",
    "    allPreds[allPreds<0] = 0\n",
    "\n",
    "    allPredictions = pd.DataFrame(allPreds, columns=['prediction'])\n",
    "\n",
    "    sample_submission['prediction'] = allPredictions['prediction']\n",
    "\n",
    "    return sample_submission\n",
    "\n",
    "#Submission \n",
    "sub = submission({'A': [catBoost_models['A']], \n",
    "                   'B': [catBoost_models['B']], \n",
    "                   'C': [catBoost_models['C']]},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder in working\n",
    "from pathlib import Path\n",
    "Path('data/final_submissions').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE SUBMISSIONS\n",
    "def saveSub(sub, filename):\n",
    "    filepath = 'data/final_submissions/' + filename + '.csv'\n",
    "    #PHD_STIPENDS = pd.read_csv('/kaggle/input/phd-stipends/csv') # load from notebook input\n",
    "    sub.to_csv(filepath, index=False) # save to notebook output\n",
    "\n",
    "saveSub(sub, \"submission_1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
