{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_18076\\3022532951.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_new = df.groupby(['date_forecast'], as_index=False).mean()\n"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "def readData(): \n",
    "    dataA, dataB, dataC = [], [], []\n",
    "    path = 'data/data/'\n",
    "    \n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'train_targets.parquet')))\n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'X_test_estimated.parquet')))\n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'X_train_estimated.parquet')))\n",
    "    dataA.append(pd.DataFrame(pd.read_parquet(path + 'A/' + 'X_train_observed.parquet')))        \n",
    "   \n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'train_targets.parquet')))\n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'X_test_estimated.parquet')))\n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'X_train_estimated.parquet')))\n",
    "    dataB.append(pd.DataFrame(pd.read_parquet(path + 'B/' + 'X_train_observed.parquet')))\n",
    "\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'train_targets.parquet')))\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'X_test_estimated.parquet')))\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'X_train_estimated.parquet')))\n",
    "    dataC.append(pd.DataFrame(pd.read_parquet(path + 'C/' + 'X_train_observed.parquet')))\n",
    "   \n",
    "    return dataA, dataB, dataC\n",
    "\n",
    "A, B, C = readData()\n",
    "\n",
    "def splitWeatherAndEnergyReports(data):\n",
    "    weather = [data[1], data[2], data[3]]\n",
    "    energy = data[0]\n",
    "    return weather, energy\n",
    "\n",
    "def quartersToHours(data):\n",
    "    data1 = []\n",
    "    for df in data:\n",
    "        df['date_forecast'] = df['date_forecast'].map(lambda x: str(x)[:-6])\n",
    "        df_new = df.groupby(['date_forecast'], as_index=False).mean()\n",
    "        df_new['date_forecast'] = df_new['date_forecast'].apply(lambda x: x + ':00:00')\n",
    "        data1.append(df_new)     \n",
    "    return data1\n",
    "\n",
    "#A\n",
    "X = pd.concat([A[3], A[2]], ignore_index = True)\n",
    "X = pd.concat([X, A[1]], ignore_index = True)\n",
    "X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "X[['ceiling_height_agl:m', 'cloud_base_agl:m']] = X[['ceiling_height_agl:m', 'cloud_base_agl:m']].interpolate(method='cubic')\n",
    "A[3], A[2], A[1] = X[:len(A[3])], X[len(A[3]):len(A[3])+len(A[2])], X[len(A[2])+len(A[3]):]\n",
    "\n",
    "\n",
    "#B\n",
    "X = pd.concat([B[3], B[2]], ignore_index = True)\n",
    "X = pd.concat([X, B[1]], ignore_index = True)\n",
    "X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "X[['ceiling_height_agl:m', 'cloud_base_agl:m']] = X[['ceiling_height_agl:m', 'cloud_base_agl:m']].interpolate(method='cubic')\n",
    "B[3], B[2], B[1] = X[:len(B[3])], X[len(B[3]):len(B[3])+len(B[2])], X[len(B[2])+len(B[3]):]\n",
    "\n",
    "\n",
    "#C\n",
    "X = pd.concat([C[3], C[2]], ignore_index = True)\n",
    "X = pd.concat([X, C[1]], ignore_index = True)\n",
    "X['snow_density:kgm3'] = X['snow_density:kgm3'].fillna(0)\n",
    "X[['ceiling_height_agl:m', 'cloud_base_agl:m']] = X[['ceiling_height_agl:m', 'cloud_base_agl:m']].interpolate(method='cubic')\n",
    "\n",
    "C[3], C[2], C[1] = X[:len(C[3])], X[len(C[3]):len(C[3])+len(C[2])], X[len(C[2])+len(C[3]):]\n",
    "\n",
    "#Splitting weather and energy datasets\n",
    "weather_A, energy_A = splitWeatherAndEnergyReports(A)\n",
    "weather_B, energy_B = splitWeatherAndEnergyReports(B)\n",
    "weather_C, energy_C = splitWeatherAndEnergyReports(C)\n",
    "\n",
    "#Joining rows from same hour.\n",
    "weather_A1 = quartersToHours(weather_A)\n",
    "weather_B1 = quartersToHours(weather_B)\n",
    "weather_C1 = quartersToHours(weather_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B - Converting rows in B that is incorrect to NAN, to later be removed. \n",
    "energy_B['flag'] = energy_B['pv_measurement'].groupby([energy_B['pv_measurement'], energy_B['pv_measurement'].diff().ne(0).cumsum()]).transform('size').ge(24).astype(int) \n",
    "energy_B.loc[energy_B['flag'] == 1, 'pv_measurement'] = None\n",
    "energy_B = energy_B.drop(['flag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#DROP NAN-ROWS\n",
    "def dropNanrows(df):\n",
    "    return df.dropna()\n",
    "\n",
    "#Block B&C has label-rows with NAN-values that needs to be dropped.\n",
    "energy_B  = dropNanrows(energy_B)\n",
    "energy_C = dropNanrows(energy_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align rows\n",
    "def alignData(x_train, x_test, y):     \n",
    "    X = pd.concat([x_train, x_test], ignore_index = True)\n",
    "    y = y.rename(columns={'time': 'date_forecast'})\n",
    "    y['date_forecast'] = y['date_forecast'].astype(str)\n",
    "    aligned = X.merge(y, how='inner', on=['date_forecast'])\n",
    "\n",
    "    X = aligned.drop(['pv_measurement'], axis=1)\n",
    "    Y = aligned['pv_measurement']\n",
    "    return X, Y\n",
    "\n",
    "#A-BLOCK\n",
    "X_A, Y_A = alignData(weather_A1[2], weather_A1[1], energy_A) \n",
    "testData_A = weather_A1[0] \n",
    "\n",
    "#B-BLOCK\n",
    "X_B, Y_B = alignData(weather_B1[2], weather_B1[1], energy_B)\n",
    "testData_B = weather_B1[0]\n",
    "\n",
    "#C-BLOCK\n",
    "X_C, Y_C = alignData(weather_C1[2], weather_C1[1], energy_C)\n",
    "testData_C = weather_C1[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing functions (currently in use)\n",
    "\n",
    "def normalizeAll(tr, te):\n",
    "    all = pd.concat([tr, te], ignore_index = True)\n",
    "    #minmax = [(all[str(col)].min(), all[str(col)].max()) for col in all.columns]\n",
    "    all = all.apply(lambda iterator: ((iterator.max() - iterator)/(iterator.max() - iterator.min())))\n",
    "    return all[:len(tr)], all[len(tr):]\n",
    "\n",
    "def dataSplit2(X_tree, X_NN, Y, td_tree, td_NN, splitRatio):\n",
    "        X_tree['Y'] = pd.DataFrame(Y)['pv_measurement']\n",
    "        tr_trees, te_trees = train_test_split(X_tree, test_size=splitRatio)\n",
    "        \n",
    "        Y_tr_trees = tr_trees['Y']\n",
    "        Y_te_trees = te_trees['Y']\n",
    "\n",
    "        tr_trees.drop(['Y'], axis=1, inplace=True)\n",
    "        te_trees.drop(['Y'], axis=1, inplace=True)\n",
    "\n",
    "        X_NN['Y'] = pd.DataFrame(Y)['pv_measurement']\n",
    "        tr_NN, te_NN = train_test_split(X_NN, test_size=splitRatio)\n",
    "        \n",
    "        Y_tr_NN = tr_NN['Y']\n",
    "        Y_te_NN = te_NN['Y']\n",
    "\n",
    "        tr_NN.drop(['Y'], axis=1, inplace=True)\n",
    "        te_NN.drop(['Y'], axis=1, inplace=True)\n",
    "\n",
    "        return tr_trees, te_trees, tr_NN, te_NN, Y_tr_trees, Y_te_trees, Y_tr_NN, Y_te_NN, td_tree, td_NN\n",
    "\n",
    "def add_times(tr, te):\n",
    "    all = pd.concat([tr, te], ignore_index = True) \n",
    "    all[\"Hour\"] = [int(all.iloc[i]['date_forecast'][11:13]) for i in range(len(all))]\n",
    "    all[\"Month\"] = [int(all.iloc[i]['date_forecast'][5:7]) for i in range(len(all))]\n",
    "    all[\"Year\"] = [int(all.iloc[i]['date_forecast'][0:4]) for i in range(len(all))]\n",
    "    all[\"Day\"] = [int(all.iloc[i]['date_forecast'][8:10]) for i in range(len(all))]\n",
    "    all['Week'] = pd.to_datetime(all['date_forecast']).dt.isocalendar().week.astype(float)\n",
    "    return all[:len(tr)], all[len(tr):] \n",
    "   \n",
    "def removeDates(X):\n",
    "    X = X.drop(['date_forecast'], axis=1)\n",
    "    return X\n",
    "\n",
    "def removeColumns(df, columns): \n",
    "    return df.drop(columns, axis=1)  \n",
    "\n",
    "def flag(tr, te, splitIndex):\n",
    "    data_observed = tr.iloc[:splitIndex]\n",
    "    data_estimated = tr.iloc[splitIndex:]\n",
    "    \n",
    "    data_observed['flag'] = 1\n",
    "    data_estimated['flag'] = 0\n",
    "    te['flag'] = 0\n",
    "    return pd.concat([data_observed, data_estimated], ignore_index = True), te\n",
    "\n",
    "def createFeatures(tr, te,): \n",
    "    all = pd.concat([tr, te], ignore_index = True)\n",
    "    top5 = ['absolute_humidity_2m:gm3', 'ceiling_height_agl:m', 'air_density_2m:kgm3', 'cloud_base_agl:m', 'sun_azimuth:d'] \n",
    "    seenKombos = []\n",
    "    toCombine = [('effective_cloud_cover:p', 'clear_sky_energy_1h:J')]\n",
    "    for elem in top5:\n",
    "        for elem2 in top5:\n",
    "            if {elem, elem2} not in seenKombos:\n",
    "               seenKombos.append({elem, elem2})\n",
    "               if elem != elem2:\n",
    "                    toCombine.append((str(elem), str(elem2)))\n",
    "    toClose = []\n",
    "    for col in all.copy().columns:\n",
    "        for col2 in all.copy().columns:\n",
    "            if {str(col), str(col2)} not in seenKombos:\n",
    "                seenKombos.append({str(col), str(col2)})\n",
    "                if col != col2:\n",
    "                    if abs(all[str(col)].corr(all[str(col2)])) >= 0.8:\n",
    "                        if str(col)[:4] == str(col2)[:4]:\n",
    "                            toClose.append((str(col), str(col2)))\n",
    "                        elif str(col) in top5 or str(col2) in top5:\n",
    "                            toCombine.append((str(col), str(col2)))                       \n",
    "    created = []\n",
    "    for i in range(len(toCombine)):\n",
    "        all[toCombine[i][0] + '_' + toCombine[i][1]] = all[toCombine[i][0]] * all[toCombine[i][1]]\n",
    "        created.append(toCombine[i][0] + '_' + toCombine[i][1])\n",
    "    return all[:len(tr)], all[len(tr):], created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_observed['flag'] = 1\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_estimated['flag'] = 0\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_observed['flag'] = 1\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_estimated['flag'] = 0\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_observed['flag'] = 1\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_10532\\2065673847.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_estimated['flag'] = 0\n"
     ]
    }
   ],
   "source": [
    "#Applying preprocessing functions to data-blocks.\n",
    "\n",
    "#DATA A\n",
    "splitIndex_A = X_A.index[X_A['date_forecast'] == '2022-10-28 22:00:00'][0]\n",
    "X_A, testData_A = flag(X_A, testData_A, splitIndex_A) #Observed data\n",
    "X_A, testData_A = add_times(X_A, testData_A)\n",
    "testData_A = removeDates(testData_A)\n",
    "X_A = removeColumns(X_A, X_A.columns.difference(testData_A.columns))  #Fitting trainingData to useful Features in testData\n",
    "X_A, testData_A, created = createFeatures(X_A, testData_A)\n",
    "X_A_neuralNets, testData_A_neuralNets = normalizeAll(X_A.copy(), testData_A.copy())\n",
    "X_A_tr_trees, X_A_te_trees,  X_A_tr_NN, X_A_te_NN, Y_A_tr_trees, Y_A_te_trees, Y_A_tr_NN, Y_A_te_NN, testData_A_tree, testData_A_NN = dataSplit2(X_A, X_A_neuralNets, Y_A, testData_A, \n",
    "                                                                                                                          testData_A_neuralNets, 0.15)\n",
    "#Features to remove (Products from createFeatures() that wasn't useful)\n",
    "X_A_tr_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_A_te_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_A_tr_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_A_te_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_A_tree.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_A_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#DATA B\n",
    "splitIndex_B = X_B.index[X_B['date_forecast'] == '2022-10-28 22:00:00'][0]\n",
    "X_B, testData_B = flag(X_B, testData_B, splitIndex_B)\n",
    "X_B, testData_B = add_times(X_B, testData_B )\n",
    "testData_B = removeDates(testData_B)\n",
    "X_B = removeColumns(X_B, X_B.columns.difference(testData_B.columns)) #Fitting trainingData to useful FEatures in testData\n",
    "X_B, testData_B, created = createFeatures(X_B, testData_B)\n",
    "X_B_neuralNets, testData_B_neuralNets = normalizeAll(X_B.copy(), testData_B.copy())\n",
    "X_B_tr_trees, X_B_te_trees,  X_B_tr_NN, X_B_te_NN, Y_B_tr_trees, Y_B_te_trees, Y_B_tr_NN, Y_B_te_NN, testData_B_tree, testData_B_NN = dataSplit2(X_B, X_B_neuralNets, Y_B, testData_B, \n",
    "                                                                                                                          testData_B_neuralNets, 0.15)\n",
    "#Features to remove (Products from createFeatures() that wasn't useful)\n",
    "X_B_tr_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_B_te_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_B_tr_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_B_te_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_B_tree.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_B_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#DATA C\n",
    "splitIndex_C = X_C.index[X_C['date_forecast'] == '2022-10-28 22:00:00'][0]\n",
    "X_C, testData_C = flag(X_C, testData_C, splitIndex_C)\n",
    "X_C, testData_C = add_times(X_C, testData_C)\n",
    "testData_C = removeDates(testData_C)\n",
    "X_C = removeColumns(X_C, X_C.columns.difference(testData_C.columns)) #Fitting trainingData to useful FEatures in testData\n",
    "X_C, testData_C, created = createFeatures(X_C, testData_C)\n",
    "X_C_neuralNets, testData_C_neuralNets = normalizeAll(X_C.copy(), testData_C.copy())\n",
    "X_C_tr_trees, X_C_te_trees,  X_C_tr_NN, X_C_te_NN, Y_C_tr_trees, Y_C_te_trees, Y_C_tr_NN, Y_C_te_NN, testData_C_tree, testData_C_NN = dataSplit2(X_C, X_C_neuralNets, Y_C, testData_C, \n",
    "                                                                                                                          testData_C_neuralNets, 0.15)\n",
    "#Features to remove (Products from createFeatures() that wasn't useful)\n",
    "X_C_tr_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_C_te_trees.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_C_tr_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "X_C_te_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_C_tree.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "testData_C_NN.drop(['absolute_humidity_2m:gm3_ceiling_height_agl:m', 'absolute_humidity_2m:gm3_dew_point_2m:K', 'absolute_humidity_2m:gm3_t_1000hPa:K'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTDATA\n",
    "testData_A = np.nan_to_num(testData_A_neuralNets.to_numpy(), nan=0)\n",
    "testData_B = np.nan_to_num(testData_B_neuralNets.to_numpy(), nan=0)\n",
    "testData_C = np.nan_to_num(testData_C_neuralNets.to_numpy(), nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:106.14328\tvalidation_1-mae:102.15755\n",
      "[1]\tvalidation_0-mae:104.05585\tvalidation_1-mae:100.10984\n",
      "[2]\tvalidation_0-mae:101.76493\tvalidation_1-mae:97.90923\n",
      "[3]\tvalidation_0-mae:99.33343\tvalidation_1-mae:95.55962\n",
      "[4]\tvalidation_0-mae:96.93792\tvalidation_1-mae:93.28134\n",
      "[5]\tvalidation_0-mae:94.49044\tvalidation_1-mae:90.90355\n",
      "[6]\tvalidation_0-mae:92.02918\tvalidation_1-mae:88.60435\n",
      "[7]\tvalidation_0-mae:89.80757\tvalidation_1-mae:86.51291\n",
      "[8]\tvalidation_0-mae:87.49907\tvalidation_1-mae:84.37232\n",
      "[9]\tvalidation_0-mae:85.23770\tvalidation_1-mae:82.28994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matse\\OneDrive - NTNU\\NTNU\\MachineLearning\\projectML_final\\ellingsensverksted_final.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m m \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(booster\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgbtree\u001b[39m\u001b[39m'\u001b[39m, objective\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mreg:absoluteerror\u001b[39m\u001b[39m'\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                             n_estimators\u001b[39m=\u001b[39m\u001b[39m20000\u001b[39m, eta\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, eval_metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m, sampling_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                              reg_lambda\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, reg_alpha\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, subsample\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m xgboost_models \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m: xgboost(X_B_tr_trees, Y_B_tr_trees, X_B_te_trees, Y_B_te_trees,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                max_depth\u001b[39m=\u001b[39;49m\u001b[39m14\u001b[39;49m, n_estimators\u001b[39m=\u001b[39;49m\u001b[39m2251\u001b[39;49m, eta\u001b[39m=\u001b[39;49m\u001b[39m0.03\u001b[39;49m, reg_lambda\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, reg_aplha\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, subsample\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, colsample_bytree\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m), \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m: xgboost(X_B_tr_trees, Y_B_tr_trees, X_B_te_trees, Y_B_te_trees,            \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                                max_depth\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, n_estimators\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m, eta\u001b[39m=\u001b[39m\u001b[39m0.03\u001b[39m, reg_lambda\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, reg_aplha\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, subsample\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m), \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: xgboost(X_B_tr_trees, Y_B_tr_trees, X_B_te_trees, Y_B_te_trees,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                max_depth\u001b[39m=\u001b[39m\u001b[39m14\u001b[39m, n_estimators\u001b[39m=\u001b[39m\u001b[39m2251\u001b[39m, eta\u001b[39m=\u001b[39m\u001b[39m0.03\u001b[39m, reg_lambda\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, reg_aplha\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, subsample\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m), \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                   }\n",
      "\u001b[1;32mc:\\Users\\matse\\OneDrive - NTNU\\NTNU\\MachineLearning\\projectML_final\\ellingsensverksted_final.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mxgboost\u001b[39m(X_tr, Y_tr, X_te, Y_te, max_depth\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_estimators\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, eta\u001b[39m=\u001b[39m\u001b[39m0.03\u001b[39m, reg_lambda\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, reg_aplha\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, subsample\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(booster\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgbtree\u001b[39m\u001b[39m'\u001b[39m, objective\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mreg:absoluteerror\u001b[39m\u001b[39m'\u001b[39m, max_depth\u001b[39m=\u001b[39mmax_depth,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             n_estimators\u001b[39m=\u001b[39mn_estimators, eta\u001b[39m=\u001b[39meta, eval_metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m, sampling_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                              reg_lambda\u001b[39m=\u001b[39mreg_lambda, reg_alpha\u001b[39m=\u001b[39mreg_aplha, subsample\u001b[39m=\u001b[39msubsample, colsample_bytree\u001b[39m=\u001b[39mcolsample_bytree)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_tr, Y_tr, eval_set\u001b[39m=\u001b[39;49m[(X_tr, Y_tr), (X_te, Y_te)], verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matse/OneDrive%20-%20NTNU/NTNU/MachineLearning/projectML_final/ellingsensverksted_final.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\matse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\matse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m (\n\u001b[0;32m   1078\u001b[0m     model,\n\u001b[0;32m   1079\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1085\u001b[0m )\n\u001b[1;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1087\u001b[0m     params,\n\u001b[0;32m   1088\u001b[0m     train_dmatrix,\n\u001b[0;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1098\u001b[0m )\n\u001b[0;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\matse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\matse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2049\u001b[0m     _check_call(\n\u001b[1;32m-> 2050\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[0;32m   2051\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[0;32m   2052\u001b[0m         )\n\u001b[0;32m   2053\u001b[0m     )\n\u001b[0;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#XGBOOST-MODEL\n",
    "def xgboost(X_tr, Y_tr, X_te, Y_te, max_depth=10, n_estimators=10, eta=0.03, reg_lambda=1, reg_aplha=0.001, subsample=0.7, colsample_bytree=0.7):\n",
    "    model = xgb.XGBRegressor(booster='gbtree', objective= 'reg:absoluteerror', max_depth=max_depth,\n",
    "                            n_estimators=n_estimators, eta=eta, eval_metric='mae', sampling_method='uniform', \n",
    "                             reg_lambda=reg_lambda, reg_alpha=reg_aplha, subsample=subsample, colsample_bytree=colsample_bytree)\n",
    "    model.fit(X_tr, Y_tr, eval_set=[(X_tr, Y_tr), (X_te, Y_te)], verbose=True)\n",
    "    return model\n",
    "\n",
    "m = xgb.XGBRegressor(booster='gbtree', objective= 'reg:absoluteerror', max_depth=7,\n",
    "                            n_estimators=20000, eta=0.1, eval_metric='mae', sampling_method='uniform', \n",
    "                             reg_lambda=1, reg_alpha=0.0001, subsample=0.7, colsample_bytree=0.8)\n",
    "\n",
    "xgboost_models = {'A': xgboost(X_B_tr_trees, Y_B_tr_trees, X_B_te_trees, Y_B_te_trees,\n",
    "                               max_depth=14, n_estimators=2251, eta=0.03, reg_lambda=1, reg_aplha=0.001, subsample=0.7, colsample_bytree=0.8), \n",
    "\n",
    "                  'B': xgboost(X_B_tr_trees, Y_B_tr_trees, X_B_te_trees, Y_B_te_trees,            \n",
    "                               max_depth=15, n_estimators=2000, eta=0.03, reg_lambda=1, reg_aplha=0.001, subsample=0.7, colsample_bytree=0.8), \n",
    "\n",
    "                  'C': xgboost(X_B_tr_trees, Y_B_tr_trees, X_B_te_trees, Y_B_te_trees,\n",
    "                               max_depth=14, n_estimators=2251, eta=0.03, reg_lambda=1, reg_aplha=0.001, subsample=0.7, colsample_bytree=0.8), \n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a23c912d8e48039bfc2842ea549f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e21c78d61e348758c300e1a9b120b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551e9cb21179434bbdc01825269dc06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CATBOOST-MODEL\n",
    "def catBoost(iterations=10, depth=14, lr=0.1,  loss =\"MAE\"):\n",
    "  cat_model = CatBoostRegressor(iterations=iterations, \n",
    "                            depth=depth, \n",
    "                            learning_rate=lr, \n",
    "                            loss_function=loss,\n",
    "                            )\n",
    "  cat_model.fit(X_B_tr_trees, Y_B_tr_trees, eval_set=(X_B_te_trees, Y_B_te_trees), verbose=False, plot=True)\n",
    "  return cat_model\n",
    "\n",
    "#Train catboost-models\n",
    "catBoost_models = {'A': catBoost(iterations=3800, depth=12, lr=0.01, loss=\"MAE\"),\n",
    "                   'B': catBoost(iterations=30000, depth=9, lr=0.001, loss=\"MAE\"),\n",
    "                   'C': catBoost(iterations=30000, depth=9, lr=0.001, loss=\"MAE\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:9: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_A = np.column_stack((m for m in models_A))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:27: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_B = np.column_stack((m for m in models_B))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:43: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_C = np.column_stack((m for m in models_C))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:9: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_A = np.column_stack((m for m in models_A))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:27: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_B = np.column_stack((m for m in models_B))\n",
      "C:\\Users\\matse\\AppData\\Local\\Temp\\ipykernel_7304\\3657094967.py:43: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  preds_C = np.column_stack((m for m in models_C))\n"
     ]
    }
   ],
   "source": [
    "#SUBMISSIONS\n",
    "def submission(dict_models):\n",
    "    sample_submission = pd.read_csv('data/data/sample_submission.csv')\n",
    "    weights = {'A': [0.8, 0.2], 'B': [0.6, 0.4], 'C': [0.7, 0.3]}\n",
    "    \n",
    "    #A  \n",
    "    models_A = [m.predict(testData_A_tree) for m in dict_models['A']]\n",
    "    preds_A = np.column_stack([m for m in models_A])\n",
    "    preds = []\n",
    "    for i in range(len(preds_A)):\n",
    "        preds.append((preds_A[i][0]*weights['A'][0] + preds_A[i][1]*weights['A'][1]))\n",
    "    weightedMeanPrediction_A = np.array(preds)\n",
    "    final_pred_A = [[p] for p in weightedMeanPrediction_A]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #B \n",
    "    models_B = [m.predict(testData_B_tree) for m in dict_models['B']]\n",
    "    preds_B = np.column_stack([m for m in models_B])\n",
    "    preds = []\n",
    "    for i in range(len(preds_B)):\n",
    "        preds.append((preds_B[i][0]*weights['B'][0] + preds_B[i][1]*weights['B'][1]))\n",
    "    weightedMeanPrediction_B = np.array(preds)\n",
    "    final_pred_B = [[p] for p in weightedMeanPrediction_B]\n",
    "   \n",
    "\n",
    "    #C \n",
    "    models_C = [m.predict(testData_C_tree) for m in dict_models['C']]\n",
    "    preds_C = np.column_stack([m for m in models_C])\n",
    "    preds = []\n",
    "    for i in range(len(preds_C)):\n",
    "        preds.append((preds_C[i][0]*weights['C'][0] + preds_C[i][1]*weights['C'][1]))\n",
    "    weightedMeanPrediction_C = np.array(preds)\n",
    "    final_pred_C = [[p] for p in weightedMeanPrediction_C]\n",
    "\n",
    "\n",
    "\n",
    "    allPreds = np.append(final_pred_A, final_pred_B, axis=0)\n",
    "    allPreds = np.append(allPreds, final_pred_C, axis=0)\n",
    "\n",
    "    #Replace potential negative values with zero, as energy production cannot be negative.\n",
    "    allPreds[allPreds<0] = 0\n",
    "\n",
    "    allPredictions = pd.DataFrame(allPreds, columns=['prediction'])\n",
    "\n",
    "    sample_submission['prediction'] = allPredictions['prediction']\n",
    "\n",
    "    return sample_submission\n",
    "\n",
    "#Submission\n",
    "sub = submission({'A': [catBoost_models['A'], xgboost_models['A']], \n",
    "                   'B': [catBoost_models['B'], xgboost_models['B']], \n",
    "                   'C': [catBoost_models['C'],  xgboost_models['C']]},)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder in working\n",
    "from pathlib import Path\n",
    "Path('data/final_submissions').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE SUBMISSIONS\n",
    "def saveSub(sub, filename):\n",
    "    filepath = 'data/final_submissions/' + filename + '.csv'\n",
    "    #PHD_STIPENDS = pd.read_csv('/kaggle/input/phd-stipends/csv') # load from notebook input\n",
    "    sub.to_csv(filepath, index=False) # save to notebook output\n",
    "\n",
    "saveSub(sub, \"submission_2\")\n",
    "#saveSub(sub2, \"submission_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
